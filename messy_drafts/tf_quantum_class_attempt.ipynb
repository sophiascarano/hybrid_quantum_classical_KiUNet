{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tf_quantum_class_attempt.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOWX7agceLPn3ZEnFY819Gd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sophiascarano/hybrid_quantum_classical_KiUNet/blob/main/messy_drafts/tf_quantum_class_attempt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JCSrz9hunJx"
      },
      "source": [
        "# try:\n",
        "#   # %tensorflow_version only exists in Colab.\n",
        "#   %tensorflow_version 2.x\n",
        "# except Exception:\n",
        "#   pass\n",
        "\n",
        "# import tensorflow as tf\n",
        "# import numpy as np\n",
        "\n",
        "# from tensorflow.keras.layers import Layer, InputSpec\n",
        "# from keras.utils import conv_utils\n",
        "# import keras.backend as K"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ur1VXPjhA8W"
      },
      "source": [
        "clean up these imports..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlBh498OLYvv"
      },
      "source": [
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "# inherit from this base class\n",
        "from tensorflow.keras.layers import Layer\n",
        "\n",
        "############\n",
        "import functools\n",
        "from keras import activations\n",
        "from keras import backend\n",
        "from keras import constraints\n",
        "from keras import initializers\n",
        "from keras import regularizers\n",
        "from tensorflow.keras.layers import Layer, InputSpec\n",
        "# imports for backwards namespace compatibility\n",
        "# pylint: disable=unused-import\n",
        "from keras.layers.pooling import AveragePooling1D\n",
        "from keras.layers.pooling import AveragePooling2D\n",
        "from keras.layers.pooling import AveragePooling3D\n",
        "from keras.layers.pooling import MaxPooling1D\n",
        "from keras.layers.pooling import MaxPooling2D\n",
        "from keras.layers.pooling import MaxPooling3D\n",
        "# pylint: enable=unused-import\n",
        "from keras.utils import conv_utils\n",
        "#from keras.utils import tf_utils\n",
        "from tensorflow.python.util.tf_export import keras_export"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VipkqHh3MkhQ"
      },
      "source": [
        "import tensorflow.compat.v2 as tf\n",
        "\n",
        "import functools\n",
        "from keras import activations\n",
        "from keras import backend\n",
        "from keras import constraints\n",
        "from keras import initializers\n",
        "from keras import regularizers\n",
        "from tensorflow.keras.layers import Layer, InputSpec\n",
        "# imports for backwards namespace compatibility\n",
        "# pylint: disable=unused-import\n",
        "from keras.layers.pooling import AveragePooling1D\n",
        "from keras.layers.pooling import AveragePooling2D\n",
        "from keras.layers.pooling import AveragePooling3D\n",
        "from keras.layers.pooling import MaxPooling1D\n",
        "from keras.layers.pooling import MaxPooling2D\n",
        "from keras.layers.pooling import MaxPooling3D\n",
        "# pylint: enable=unused-import\n",
        "from keras.utils import conv_utils\n",
        "#from keras.utils import tf_utils\n",
        "from tensorflow.python.util.tf_export import keras_export"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTE8w95y8l9P"
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import functools\n",
        "import numbers\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.python.eager import context\n",
        "from tensorflow.python.framework import constant_op\n",
        "from tensorflow.python.framework import dtypes\n",
        "from tensorflow.python.framework import errors_impl\n",
        "from tensorflow.python.framework import graph_util\n",
        "from tensorflow.python.framework import ops\n",
        "from tensorflow.python.framework import random_seed\n",
        "from tensorflow.python.framework import tensor_shape\n",
        "from tensorflow.python.framework import tensor_util\n",
        "from tensorflow.python.ops import array_ops\n",
        "from tensorflow.python.ops import check_ops\n",
        "from tensorflow.python.ops import gen_math_ops\n",
        "from tensorflow.python.ops import gen_nn_ops\n",
        "from tensorflow.python.ops import math_ops\n",
        "from tensorflow.python.ops import random_ops\n",
        "from tensorflow.python.ops import variables as variables_lib\n",
        "# go/tf-wildcard-import\n",
        "# pylint: disable=wildcard-import\n",
        "from tensorflow.python.ops.gen_nn_ops import *\n",
        "# pylint: enable=wildcard-import\n",
        "from tensorflow.python.platform import device_context\n",
        "from tensorflow.python.util import deprecation\n",
        "from tensorflow.python.util import dispatch\n",
        "from tensorflow.python.util.compat import collections_abc\n",
        "from tensorflow.python.util.deprecation import deprecated_args\n",
        "from tensorflow.python.util.deprecation import deprecated_argument_lookup\n",
        "\n",
        "from tensorflow.python.util.tf_export import tf_export"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPMAWDIFhEr0"
      },
      "source": [
        "end notes:\n",
        "- in the documentation, it seems that these helper functions go in a loop.\n",
        "- also the eager mode / numpy compat makes this unnecessary anyway. Will have to attempt in pytorch next."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNIWwMvpqtfW"
      },
      "source": [
        "class QConv2D(Layer):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    filters: Integer, the dimensionality of the output space (i.e. the number\n",
        "      of filters in the convolution).\n",
        "    kernel_size: An integer or tuple/list of 2 integers, specifying the\n",
        "      dimensions of the convolution window. (width, height)\n",
        "    groups: A positive integer specifying the number of groups in which the\n",
        "      input is split along the channel axis. Each group is convolved\n",
        "      separately with `filters / groups` filters. The output is the\n",
        "      concatenation of all the `groups` results along the channel axis.\n",
        "      Input channels and `filters` must both be divisible by `groups`. So depth.\n",
        "    strides: An integer or tuple/list of n integers, specifying the stride \n",
        "      length of the convolution.\n",
        "    padding: One of `\"valid\"`, or  `\"same\"` (case-insensitive).\n",
        "      `\"valid\"` means no padding. `\"same\"` results in padding with zeros\n",
        "      evenly to the left/right or up/down of the input such that output has the\n",
        "      same height/width dimension as the input.\n",
        "    data_format:  data_format: A string, one of `channels_last` (default) or `channels_first`.\n",
        "      The ordering of the dimensions in the inputs.\n",
        "      `channels_last` corresponds to inputs with shape\n",
        "      `(batch_size, ..., channels)` while `channels_first` corresponds to\n",
        "      inputs with shape `(batch_size, channels, ...)`.\n",
        "    \n",
        "  \"\"\"\n",
        "  def __init__(self,\n",
        "               filters,\n",
        "               kernel_size,\n",
        "               groups=1,\n",
        "               strides=1,\n",
        "               padding='same',\n",
        "               rank=2,\n",
        "               data_format='channels_last',\n",
        "               **kwargs\n",
        "               ):\n",
        "    super(QConv2D, self).__init__(**kwargs)\n",
        "    self.rank = rank\n",
        "\n",
        "    if isinstance(filters, float):\n",
        "      filters = int(filters)\n",
        "    self.filters = filters\n",
        "    self.groups = groups or 1\n",
        "    self.kernel_size = conv_utils.normalize_tuple(kernel_size, \n",
        "                                                  rank, \n",
        "                                                  'kernel_size')\n",
        "    self.strides = conv_utils.normalize_tuple(strides, rank, 'strides')\n",
        "    self.padding = conv_utils.normalize_padding(padding)\n",
        "    #self.data_format = conv_utils.normalize_data_format(data_format) #discontinued\n",
        "    self.data_format = self._normalize_data_format(data_format)\n",
        "    #self.data_format = K.normalize_data_format(data_format)\n",
        "    self.input_spec = InputSpec(min_ndim=self.rank+2) #not sure why? but whatever\n",
        "\n",
        "    self._validate_init()\n",
        "    self._channels_first = self.data_format == 'channels_first'\n",
        "    self._tf_data_format = self._convert_data_format(\n",
        "        self.data_format, self.rank + 2)\n",
        "\n",
        "  def _validate_init(self):\n",
        "    if self.filters is not None and self.filters % self.groups != 0:\n",
        "      raise ValueError(\n",
        "          'The number of filters must be evenly divisible by the number of '\n",
        "          'groups. Received: groups={}, filters={}'.format(\n",
        "              self.groups, self.filters))\n",
        "\n",
        "    if not all(self.kernel_size):\n",
        "      raise ValueError('The argument `kernel_size` cannot contain 0(s). '\n",
        "                      'Received: %s' % (self.kernel_size,))\n",
        "\n",
        "    if not all(self.strides):\n",
        "      raise ValueError('The argument `strides` cannot contains 0(s). '\n",
        "                      'Received: %s' % (self.strides,))\n",
        "      \n",
        "  def build(self, input_shape):\n",
        "    input_shape = tf.TensorShape(input_shape)\n",
        "    # input_shape = (1, input_shape[0], input_shape[1], input_shape[2])\n",
        "    # input_shape = tf.shape(input_shape)\n",
        "    input_channel = self._get_input_channel(input_shape)\n",
        "    if input_channel % self.groups !=0:\n",
        "      raise ValueError(\n",
        "          'The number of input channels must be evenly divisible by the number '\n",
        "          'of groups. Received groups={}, but the input has {} channels '\n",
        "          '(full input shape is {}).'.format(self.groups, input_channel,\n",
        "                                             input_shape))\n",
        "    kernel_shape = self.kernel_size + (input_channel // self.groups, \n",
        "                                       self.filters)\n",
        "    \n",
        "    #their version of adding variables\n",
        "    # self.kernel = self.add_weight( #from Layer class\n",
        "    #     name='kernel',\n",
        "    #     shape=kernel_shape,\n",
        "    #     initializer=self.kernel_initializer,\n",
        "    #     regularizer=self.kernel_regularizer,\n",
        "    #     constraint=self.kernel_constraint,\n",
        "    #     trainable=True,\n",
        "    #     dtype=self.dtype)\n",
        "    \n",
        "    #using tf.Variable\n",
        "    # initialize the weights\n",
        "    w_init = tf.random_normal_initializer()\n",
        "    self.kernel = tf.Variable(name=\"kernel\",\n",
        "        initial_value=w_init(shape=(kernel_shape), #not sure if shape is correct\n",
        "                              dtype='float32'),\n",
        "        trainable=True)\n",
        "  \n",
        "    #back to the example\n",
        "    channel_axis = self._get_channel_axis()\n",
        "    self.input_spec = InputSpec(min_ndim=self.rank + 2,\n",
        "                                axes={channel_axis: input_channel})\n",
        "    \n",
        "    # Convert Keras formats to TF native formats\n",
        "    if isinstance(self.padding, str):\n",
        "      tf_padding = self.padding.upper()\n",
        "    else:\n",
        "      tf_padding = self.padding\n",
        "    tf_strides = list(self.strides)\n",
        "    #ignored tf_dilations\n",
        "    \n",
        "    ''' I MAY NOT WANT TO GO THROUGH TF CONVOLUTION\n",
        "    ### lines 1120-1129 of https://github.com/tensorflow/tensorflow/blob/a4dfb8d1a71385bd6d122e4f27f86dcebb96712d/tensorflow/python/ops/nn_ops.py#L1028\n",
        "    ###   explain this and why you only have to do it for conv1d\n",
        "    tf_op_name = self.__class__.__name__\n",
        "    if tf_op_name == 'Conv1D':\n",
        "      tf_op_name = 'conv1d'  # Backwards compat.\n",
        "\n",
        "    self._convolution_op = functools.partial(\n",
        "        tf.nn.convolution,\n",
        "        strides=tf_strides,\n",
        "        padding=tf_padding,\n",
        "        dilations=tf_dilations,\n",
        "        data_format=self._tf_data_format,\n",
        "        name=tf_op_name)\n",
        "    self.built = True\n",
        "    '''\n",
        "    #tf_op_name = self.__class__.__name__\n",
        "    tf_op_name = 'Conv2D'\n",
        "\n",
        "    self._convolution_op = functools.partial(\n",
        "        my_quantum_conv,\n",
        "        strides=tf_strides,\n",
        "        padding=tf_padding,\n",
        "        #dilations=tf_dilations,\n",
        "        data_format=self._tf_data_format,\n",
        "        name=tf_op_name)\n",
        "    self.built = True\n",
        "  \n",
        "  def call(self, inputs):\n",
        "    input_shape = inputs.shape\n",
        "    outputs = self._convolution_op(inputs, self.kernel)\n",
        "    return outputs\n",
        "\n",
        "  #other helpful funcs\n",
        "\n",
        "  def _normalize_data_format(self, value):\n",
        "    '''\n",
        "    Instead of conv_utils.normalize_data_format(data_format) above\n",
        "    Found solution on https://github.com/RaphaelMeudec/deblur-gan/issues/35\n",
        "      from user sailfish009\n",
        "    '''\n",
        "    if value is None:\n",
        "      value = K.image_data_format()\n",
        "    data_format = value.lower()\n",
        "    if data_format not in {'channels_first', 'channels_last'}:\n",
        "      raise ValueError('The `data_format` argument must be one of '\n",
        "                         '\"channels_first\", \"channels_last\". Received: ' +\n",
        "                         str(value))\n",
        "    return data_format\n",
        "\n",
        "  def _convert_data_format(self, data_format, ndim):\n",
        "    #line 25 https://github.com/keras-team/keras/blob/master/keras/utils/conv_utils.py\n",
        "    if data_format == 'channels_last':\n",
        "      if ndim == 3:\n",
        "        return 'NWC'\n",
        "      elif ndim == 4:\n",
        "        return 'NHWC'\n",
        "      elif ndim == 5:\n",
        "        return 'NDHWC'\n",
        "      else:\n",
        "        raise ValueError('Input rank not supported:', ndim)\n",
        "    elif data_format == 'channels_first':\n",
        "      if ndim == 3:\n",
        "        return 'NCW'\n",
        "      elif ndim == 4:\n",
        "        return 'NCHW'\n",
        "      elif ndim == 5:\n",
        "        return 'NCDHW'\n",
        "      else:\n",
        "        raise ValueError('Input rank not supported:', ndim)\n",
        "    else:\n",
        "      raise ValueError('Invalid data_format:', data_format)\n",
        "\n",
        "  def _get_channel_axis(self):\n",
        "    if self.data_format == 'channels_first':\n",
        "      return -1 - self.rank\n",
        "    else:\n",
        "      return -1\n",
        "  \n",
        "  def _get_input_channel(self, input_shape):\n",
        "    channel_axis = self._get_channel_axis()\n",
        "    if input_shape.dims[channel_axis].value is None:\n",
        "      raise ValueError('The channel dimension of the inputs '\n",
        "                       'should be defined. Found `None`.')\n",
        "    return int(input_shape[channel_axis])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEZ6BroqAbp5"
      },
      "source": [
        "def matrix_mult(image_matrix, filter_matrix):\n",
        "  return image_matrix * filter_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I63M80OI_x14"
      },
      "source": [
        "def convolution_op(one_input, one_filter):\n",
        "  '''takes one image and convolves one filter around that image, returning feature map'''\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFtl3ZJz3B5a"
      },
      "source": [
        "def my_quantum_conv(inputs, \n",
        "                    filters, \n",
        "                    strides=1, \n",
        "                    padding='VALID', \n",
        "                    data_format='NHWC',\n",
        "                    dilations=None, \n",
        "                    name='QConv2D'):\n",
        "  return inputs\n",
        "  # return tf.nn.convolution(input, \n",
        "  #                          filters, \n",
        "  #                          strides=strides, \n",
        "  #                          padding=padding, \n",
        "  #                          data_format=data_format,\n",
        "  #                          dilations=dilations, \n",
        "  #                          name='Conv2D')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CYvS2hiJ2ME"
      },
      "source": [
        "# identity block within residual\n",
        "class IdentityBlock(tf.keras.Model):\n",
        "  def __init__(self, filters, kernel_size):\n",
        "    super(IdentityBlock, self).__init__(name='')\n",
        "\n",
        "\n",
        "    self.conv1 = QConv2D(filters, kernel_size, groups=1, strides=1, padding='same', rank=2, data_format=None)\n",
        "    self.bn1 = tf.keras.layers.BatchNormalization()\n",
        "\n",
        "  def call(self, input_tensor):\n",
        "    x = self.conv1(input_tensor)\n",
        "    x = self.bn1(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "class ResNetMini(tf.keras.Model):\n",
        "  def __init__(self, num_classes):\n",
        "    super(ResNetMini, self).__init__()\n",
        "    self.conv = tf.keras.layers.Conv2D(64, 7, padding='same')\n",
        "    self.bn = tf.keras.layers.BatchNormalization()\n",
        "    self.activation = tf.keras.layers.Activation('relu')\n",
        "    self.max_pool = tf.keras.layers.MaxPool2D((3,3))\n",
        "    self.identity1a = IdentityBlock(64, 3)\n",
        "    self.identity1b = IdentityBlock(64, 3)\n",
        "    self.global_pool = tf.keras.layers.GlobalAveragePooling2D()\n",
        "    self.classifier = tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "\n",
        "  def call(self, inputs):\n",
        "    # x = self.conv(inputs)\n",
        "    # x = self.bn(x)\n",
        "    # x = self.activation(x)\n",
        "    # x = self.max_pool(x)\n",
        "\n",
        "    # x = self.identity1a(x)\n",
        "    # x = self.identity1b(x)\n",
        "\n",
        "    x = self.identity1a(inputs)\n",
        "\n",
        "    x = self.global_pool(x)\n",
        "    x = self.classifier(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "# utility function to normalize the images and return (image, label) pairs.\n",
        "def preprocess(features):\n",
        "    return tf.cast(features['image'], tf.float32) / 255., features['label']\n",
        "\n",
        "# now actually run it I guess?\n",
        "\n",
        "resnet_mini = ResNetMini(10)\n",
        "resnet_mini.compile(optimizer='adam', loss='sparse_categorical_crossentropy',\n",
        "                    metrics=['accuracy'])\n",
        "dataset = tfds.load('mnist', split=tfds.Split.TRAIN)\n",
        "dataset = dataset.map(preprocess).batch(32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0x-txwDGO5L",
        "outputId": "b8b04c22-e3a3-471e-ebc3-211d846d37c9"
      },
      "source": [
        "resnet_mini.fit(dataset, epochs=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['res_net_mini_2/identity_block_4/q_conv2d_4/kernel:0'] when minimizing the loss.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['res_net_mini_2/identity_block_4/q_conv2d_4/kernel:0'] when minimizing the loss.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['res_net_mini_2/identity_block_4/q_conv2d_4/kernel:0'] when minimizing the loss.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['res_net_mini_2/identity_block_4/q_conv2d_4/kernel:0'] when minimizing the loss.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1875/1875 [==============================] - 14s 7ms/step - loss: 2.2686 - accuracy: 0.1353\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fee68d40f90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2oCREe2440h3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebn1d1EvGOou"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eo7WAaY9PRmn"
      },
      "source": [
        "# maybe this is the correct conv2d?\n",
        "from tensorflow.python.keras import backend_config\n",
        "image_data_format = backend_config.image_data_format\n",
        "def conv2d(x,\n",
        "           kernel,\n",
        "           strides=(1, 1),\n",
        "           padding='valid',\n",
        "           data_format=None,\n",
        "           dilation_rate=(1, 1)):\n",
        "  \n",
        "  if data_format is None:\n",
        "    data_format = image_data_format() #imported above\n",
        "  if data_format not in {'channels_first', 'channels_last'}:\n",
        "    raise ValueError('Unknown data_format: ' + str(data_format))\n",
        "\n",
        "  x, tf_data_format = _preprocess_conv2d_input(x, data_format) #defined below\n",
        "  padding = _preprocess_padding(padding) #defined below\n",
        "  x = nn.convolution( #WHAT ARE YOU KIDDING\n",
        "      input=x,\n",
        "      filter=kernel,\n",
        "      dilation_rate=dilation_rate,\n",
        "      strides=strides,\n",
        "      padding=padding,\n",
        "      data_format=tf_data_format)\n",
        "  if data_format == 'channels_first' and tf_data_format == 'NHWC':\n",
        "    x = array_ops.transpose(x, (0, 3, 1, 2))  # NHWC -> NCHW\n",
        "  return x\n",
        "\n",
        "def _preprocess_conv2d_input(x, data_format, force_transpose=False):\n",
        "  tf_data_format = 'NHWC'\n",
        "  if data_format == 'channels_first':\n",
        "    if not _has_nchw_support() or force_transpose:\n",
        "      x = array_ops.transpose(x, (0, 2, 3, 1))  # NCHW -> NHWC\n",
        "    else:\n",
        "      tf_data_format = 'NCHW'\n",
        "  return x, tf_data_format\n",
        "\n",
        "def _preprocess_padding(padding):\n",
        "  if padding == 'same':\n",
        "    padding = 'SAME'\n",
        "  elif padding == 'valid':\n",
        "    padding = 'VALID'\n",
        "  else:\n",
        "    raise ValueError('Invalid padding: ' + str(padding))\n",
        "  return padding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7QlzqP18lt2"
      },
      "source": [
        "_CHANNELS_LAST_FORMATS = frozenset({\n",
        "    \"NWC\", \"NHC\", \"NHWC\", \"NWHC\", \"NDHWC\", \"NDWHC\", \"NHDWC\", \"NHWDC\", \"NWDHC\",\n",
        "    \"NWHDC\"\n",
        "})\n",
        "\n",
        "def convolution(\n",
        "    input,  # pylint: disable=redefined-builtin\n",
        "    filter,  # pylint: disable=redefined-builtin\n",
        "    padding,\n",
        "    strides=None,\n",
        "    dilation_rate=None,\n",
        "    name=None,\n",
        "    data_format=None,\n",
        "    filters=None,\n",
        "    dilations=None):  # pylint: disable=g-doc-args\n",
        "\n",
        "  filter = deprecated_argument_lookup(\"filters\", filters, \"filter\", filter)\n",
        "  dilation_rate = deprecated_argument_lookup(\n",
        "      \"dilations\", dilations, \"dilation_rate\", dilation_rate)\n",
        "  return convolution_internal(\n",
        "      input,\n",
        "      filter,\n",
        "      strides=strides,\n",
        "      padding=padding,\n",
        "      data_format=data_format,\n",
        "      dilations=dilation_rate,\n",
        "      name=name)\n",
        "\n",
        "def convolution_internal(\n",
        "    input,  # pylint: disable=redefined-builtin\n",
        "    filters,\n",
        "    strides=None,\n",
        "    padding=\"VALID\",\n",
        "    data_format=None,\n",
        "    dilations=None,\n",
        "    name=None,\n",
        "    call_from_convolution=True,\n",
        "    num_spatial_dims=None):\n",
        "  \n",
        "  filters = ops.convert_to_tensor(filters, name='filters')\n",
        "  input = ops.convert_to_tensor(input, name=\"input\")\n",
        "\n",
        "  filters_rank = filters.shape.rank\n",
        "  inputs_rank = input.shape.rank\n",
        "\n",
        "  if num_spatial_dims is None:\n",
        "    if filters_rank:\n",
        "      num_spatial_dims = filters_rank - 2\n",
        "    elif inputs_rank:\n",
        "      num_spatial_dims = inputs_rank - 2\n",
        "    else:\n",
        "      raise ValueError(\"rank of input or filter must be known\")\n",
        "  elif filters_rank and filters_rank - 2 != num_spatial_dims:\n",
        "    raise ValueError(\n",
        "        \"inconsistent estimate of spatial dims ({}) vs. actual passed \"\n",
        "        \"num_spatial_dims ({}).  n was estimated as len(filters.shape) - 2, \"\n",
        "        \"but filters shape is: {}\".format(filters_rank, num_spatial_dims,\n",
        "                                          filters.shape))\n",
        "\n",
        "  if inputs_rank:\n",
        "    num_batch_dims = inputs_rank - num_spatial_dims - 1  # Channel dimension.\n",
        "  else:\n",
        "    num_batch_dims = 1  # By default, assume single batch dimension.\n",
        "\n",
        "  if num_spatial_dims not in {1, 2, 3}:\n",
        "    raise ValueError(\n",
        "        \"num_spatial_dims (input.shape.ndims - num_batch_dims - 1) must be one \"\n",
        "        \"of 1, 2 or 3 but saw {}.  num_batch_dims: {}.\".format(\n",
        "            num_spatial_dims, num_batch_dims))\n",
        "  \n",
        "  if data_format is None or data_format in _CHANNELS_LAST_FORMATS: #defined above\n",
        "    channel_index = num_batch_dims + num_spatial_dims\n",
        "  else:\n",
        "    channel_index = num_batch_dims\n",
        "\n",
        "  ############using _get_sequence for dilations stuff\n",
        "  if dilations is None:\n",
        "    dilations = _get_sequence(dilations, num_spatial_dims, channel_index,\n",
        "                              \"dilations\")\n",
        "    is_dilated_conv = False\n",
        "  else:\n",
        "    dilations = _get_sequence(dilations, num_spatial_dims, channel_index,\n",
        "                              \"dilations\")\n",
        "    is_dilated_conv = any(i != 1 for i in dilations)\n",
        "  ##########\n",
        "  strides = _get_sequence(strides, num_spatial_dims, channel_index, \"strides\")\n",
        "  '''\n",
        "  has_tpu_context = device_context.enclosing_tpu_context() is not None\n",
        "  '''\n",
        "  if name:\n",
        "    default_name = None\n",
        "  '''\n",
        "  elif not has_tpu_context or call_from_convolution:\n",
        "    default_name = \"convolution\" '''\n",
        "  elif num_spatial_dims == 2:  # Most common case.\n",
        "    default_name = \"Conv2D\"\n",
        "  elif num_spatial_dims == 3:\n",
        "    default_name = \"Conv3D\"\n",
        "  else:\n",
        "    default_name = \"conv1d\"\n",
        "\n",
        "  with ops.name_scope(name, default_name, [input, filters]) as name:\n",
        "    # Fast path for TPU or if no dilation, as gradient only supported on TPU\n",
        "    # for dilations.\n",
        "    if not is_dilated_conv: ''' or has_tpu_context:'''\n",
        "      if num_spatial_dims == 2:  # Most common case.\n",
        "        op = _conv2d_expanded_batch #defined below\n",
        "      '''elif num_spatial_dims == 3:\n",
        "        op = _conv3d_expanded_batch'''\n",
        "      else:\n",
        "        '''op = conv1d'''\n",
        "        raise ValueError('num_spatial_dims isnt equal to 2')\n",
        "\n",
        "      return op(\n",
        "          input,\n",
        "          filters,\n",
        "          strides,\n",
        "          padding=padding,\n",
        "          data_format=data_format,\n",
        "          dilations=dilations,\n",
        "          name=name)\n",
        "    else:\n",
        "      '''if channel_index == 1:\n",
        "        strides = strides[2:]\n",
        "        dilations = dilations[2:]\n",
        "      else:\n",
        "        strides = strides[1:-1]\n",
        "        dilations = dilations[1:-1]\n",
        "\n",
        "      op = Convolution(\n",
        "          tensor_shape.as_shape(input.shape),\n",
        "          tensor_shape.as_shape(filters.shape),\n",
        "          padding,\n",
        "          strides=strides,\n",
        "          dilation_rate=dilations,\n",
        "          name=name,\n",
        "          data_format=data_format,\n",
        "          num_spatial_dims=num_spatial_dims)\n",
        "      return op(input, filters)'''\n",
        "      raise ValueError('check dilations issue in convolution_internal')\n",
        "\n",
        "\n",
        "def _get_sequence(value, n, channel_index, name):\n",
        "  \"\"\"Formats a value input for gen_nn_ops.\"\"\"\n",
        "  # Performance is fast-pathed for common cases:\n",
        "  # `None`, `list`, `tuple` and `int`.\n",
        "  if value is None:\n",
        "    return [1] * (n + 2)\n",
        "\n",
        "  # Always convert `value` to a `list`.\n",
        "  if isinstance(value, list):\n",
        "    pass\n",
        "  elif isinstance(value, tuple):\n",
        "    value = list(value)\n",
        "  elif isinstance(value, int):\n",
        "    value = [value]\n",
        "  elif not isinstance(value, collections_abc.Sized):\n",
        "    value = [value]\n",
        "  else:\n",
        "    value = list(value)  # Try casting to a list.\n",
        "\n",
        "  len_value = len(value)\n",
        "\n",
        "  # Fully specified, including batch and channel dims.\n",
        "  if len_value == n + 2:\n",
        "    return value\n",
        "\n",
        "  # Apply value to spatial dims only.\n",
        "  if len_value == 1:\n",
        "    value = value * n  # Broadcast to spatial dimensions.\n",
        "  elif len_value != n:\n",
        "    raise ValueError(\"{} should be of length 1, {} or {} but was {}\".format(\n",
        "        name, n, n + 2, len_value))\n",
        "\n",
        "  # Add batch and channel dims (always 1).\n",
        "  if channel_index == 1:\n",
        "    return [1, 1] + value\n",
        "  else:\n",
        "    return [1] + value + [1]\n",
        "\n",
        "def _conv2d_expanded_batch(\n",
        "    input,  # pylint: disable=redefined-builtin\n",
        "    filters,\n",
        "    strides,\n",
        "    padding,\n",
        "    data_format,\n",
        "    dilations,\n",
        "    name):\n",
        "  \"\"\"Helper function for `convolution_internal`; handles expanded batches.\"\"\"\n",
        "  # Try really hard to avoid modifying the legacy name scopes - return early.\n",
        "  '''input_rank = input.shape.rank\n",
        "  if input_rank is None or input_rank < 5:'''\n",
        "    # We avoid calling squeeze_batch_dims to reduce extra python function\n",
        "    # call slowdown in eager mode.  This branch doesn't require reshapes.\n",
        "  return conv2d(\n",
        "      input,\n",
        "      filter=filters,\n",
        "      strides=strides,\n",
        "      padding=padding,\n",
        "      data_format=data_format,\n",
        "      dilations=dilations,\n",
        "      name=name)\n",
        "  '''return squeeze_batch_dims(\n",
        "      input,\n",
        "      functools.partial(\n",
        "          conv2d,\n",
        "          filter=filters,\n",
        "          strides=strides,\n",
        "          padding=padding,\n",
        "          data_format=data_format,\n",
        "          dilations=dilations),\n",
        "      inner_rank=3,\n",
        "      name=name)'''\n",
        "\n",
        "def conv2d(inputs,\n",
        "           filters,\n",
        "           kernel_size,\n",
        "           strides=(1, 1),\n",
        "           padding='valid',\n",
        "           data_format='channels_last',\n",
        "           dilation_rate=(1, 1),\n",
        "           activation=None,\n",
        "           use_bias=True,\n",
        "           kernel_initializer=None,\n",
        "           bias_initializer=init_ops.zeros_initializer(),\n",
        "           kernel_regularizer=None,\n",
        "           bias_regularizer=None,\n",
        "           activity_regularizer=None,\n",
        "           kernel_constraint=None,\n",
        "           bias_constraint=None,\n",
        "           trainable=True,\n",
        "           name=None,\n",
        "           reuse=None):\n",
        "  warnings.warn('`tf.layers.conv2d` is deprecated and '\n",
        "                'will be removed in a future version. '\n",
        "                'Please Use `tf.keras.layers.Conv2D` instead.')\n",
        "  layer = Conv2D(\n",
        "      filters=filters,\n",
        "      kernel_size=kernel_size,\n",
        "      strides=strides,\n",
        "      padding=padding,\n",
        "      data_format=data_format,\n",
        "      dilation_rate=dilation_rate,\n",
        "      activation=activation,\n",
        "      use_bias=use_bias,\n",
        "      kernel_initializer=kernel_initializer,\n",
        "      bias_initializer=bias_initializer,\n",
        "      kernel_regularizer=kernel_regularizer,\n",
        "      bias_regularizer=bias_regularizer,\n",
        "      activity_regularizer=activity_regularizer,\n",
        "      kernel_constraint=kernel_constraint,\n",
        "      bias_constraint=bias_constraint,\n",
        "      trainable=trainable,\n",
        "      name=name,\n",
        "      _reuse=reuse,\n",
        "      _scope=name)\n",
        "  return layer.apply(inputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6UPsjhqJn5I",
        "outputId": "7007e618-8412-4617-a49c-7a30190f716f"
      },
      "source": [
        "\n",
        "# utility function to normalize the images and return (image, label) pairs.\n",
        "def preprocess(features):\n",
        "    return tf.cast(features['image'], tf.float32) / 255., features['label']\n",
        "\n",
        "# now actually run it I guess?\n",
        "\n",
        "resnet_mini = ResNetMini(10)\n",
        "resnet_mini.compile(optimizer='adam', loss='sparse_categorical_crossentropy',\n",
        "                    metrics=['accuracy'])\n",
        "dataset = tfds.load('mnist', split=tfds.Split.TRAIN)\n",
        "dataset = dataset.map(preprocess).batch(32)\n",
        "resnet_mini.fit(dataset, epochs=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.3342 - accuracy: 0.9076\n",
            "Epoch 2/2\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0837 - accuracy: 0.9756\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f2da2ddbc10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4etfqtFhPDB",
        "outputId": "4c88c240-423b-4e58-92ad-e2d64a72f2b0"
      },
      "source": [
        "# Training parameters\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 1\n",
        "\n",
        "# The data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "x_train = x_train.reshape(-1, 784)\n",
        "x_test = x_test.reshape(-1, 784)\n",
        "x_train = x_train.astype(\"float32\")\n",
        "x_test = x_test.astype(\"float32\")\n",
        "x_train /= 255\n",
        "x_test /= 255"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 647
        },
        "id": "8uIZWgdIhO1S",
        "outputId": "2701e6dc-c32b-4000-b293-64ca891ade9f"
      },
      "source": [
        "resnet_mini = ResNetMini(10)\n",
        "resnet_mini.compile(optimizer='adam', loss='sparse_categorical_crossentropy',\n",
        "                    metrics=['accuracy'])\n",
        "#dataset = tfds.load('mnist', split=tfds.Split.TRAIN)\n",
        "#dataset = dataset.map(preprocess).batch(32)\n",
        "resnet_mini.fit(x_train, y_train, epochs=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-d01e9a1fe38a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#dataset = tfds.load('mnist', split=tfds.Split.TRAIN)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#dataset = dataset.map(preprocess).batch(32)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mresnet_mini\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    869\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    724\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    725\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 726\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2967\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2968\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2969\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2970\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3204\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3205\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3206\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3207\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3208\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    975\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 977\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    978\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n        return step_function(self, iterator)\n    <ipython-input-44-01910bd3638a>:19 call  *\n        x = self.identity1a(inputs)\n    <ipython-input-6-afe6586ca6c7>:11 call  *\n        x = self.conv1(input_tensor)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py:998 __call__  **\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/input_spec.py:239 assert_input_compatibility\n        str(tuple(shape)))\n\n    ValueError: Input 0 of layer q_conv2d_24 is incompatible with the layer: : expected min_ndim=4, found ndim=2. Full shape received: (32, 784)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qY74r0FB-fxE"
      },
      "source": [
        "import tensorflow.compat.v2 as tf\n",
        "\n",
        "import functools\n",
        "from keras import activations\n",
        "from keras import backend\n",
        "from keras import constraints\n",
        "from keras import initializers\n",
        "from keras import regularizers\n",
        "from keras.engine.base_layer import Layer\n",
        "from keras.engine.input_spec import InputSpec\n",
        "# imports for backwards namespace compatibility\n",
        "# pylint: disable=unused-import\n",
        "from keras.layers.pooling import AveragePooling1D\n",
        "from keras.layers.pooling import AveragePooling2D\n",
        "from keras.layers.pooling import AveragePooling3D\n",
        "from keras.layers.pooling import MaxPooling1D\n",
        "from keras.layers.pooling import MaxPooling2D\n",
        "from keras.layers.pooling import MaxPooling3D\n",
        "# pylint: enable=unused-import\n",
        "from keras.utils import conv_utils\n",
        "from keras.utils import tf_utils\n",
        "from tensorflow.python.util.tf_export import keras_export"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cikGjFSr3iG"
      },
      "source": [
        "@keras_export('keras.layers.Conv2D', 'keras.layers.Convolution2D')\n",
        "class Conv2D(Conv):\n",
        "  \"\"\"2D convolution layer (e.g. spatial convolution over images).\n",
        "  This layer creates a convolution kernel that is convolved\n",
        "  with the layer input to produce a tensor of\n",
        "  outputs. If `use_bias` is True,\n",
        "  a bias vector is created and added to the outputs. Finally, if\n",
        "  `activation` is not `None`, it is applied to the outputs as well.\n",
        "  When using this layer as the first layer in a model,\n",
        "  provide the keyword argument `input_shape`\n",
        "  (tuple of integers or `None`, does not include the sample axis),\n",
        "  e.g. `input_shape=(128, 128, 3)` for 128x128 RGB pictures\n",
        "  in `data_format=\"channels_last\"`. You can use `None` when\n",
        "  a dimension has variable size.\n",
        "  Examples:\n",
        "  >>> # The inputs are 28x28 RGB images with `channels_last` and the batch\n",
        "  >>> # size is 4.\n",
        "  >>> input_shape = (4, 28, 28, 3)\n",
        "  >>> x = tf.random.normal(input_shape)\n",
        "  >>> y = tf.keras.layers.Conv2D(\n",
        "  ... 2, 3, activation='relu', input_shape=input_shape[1:])(x)\n",
        "  >>> print(y.shape)\n",
        "  (4, 26, 26, 2)\n",
        "  >>> # With `dilation_rate` as 2.\n",
        "  >>> input_shape = (4, 28, 28, 3)\n",
        "  >>> x = tf.random.normal(input_shape)\n",
        "  >>> y = tf.keras.layers.Conv2D(\n",
        "  ... 2, 3, activation='relu', dilation_rate=2, input_shape=input_shape[1:])(x)\n",
        "  >>> print(y.shape)\n",
        "  (4, 24, 24, 2)\n",
        "  >>> # With `padding` as \"same\".\n",
        "  >>> input_shape = (4, 28, 28, 3)\n",
        "  >>> x = tf.random.normal(input_shape)\n",
        "  >>> y = tf.keras.layers.Conv2D(\n",
        "  ... 2, 3, activation='relu', padding=\"same\", input_shape=input_shape[1:])(x)\n",
        "  >>> print(y.shape)\n",
        "  (4, 28, 28, 2)\n",
        "  >>> # With extended batch shape [4, 7]:\n",
        "  >>> input_shape = (4, 7, 28, 28, 3)\n",
        "  >>> x = tf.random.normal(input_shape)\n",
        "  >>> y = tf.keras.layers.Conv2D(\n",
        "  ... 2, 3, activation='relu', input_shape=input_shape[2:])(x)\n",
        "  >>> print(y.shape)\n",
        "  (4, 7, 26, 26, 2)\n",
        "  Args:\n",
        "    filters: Integer, the dimensionality of the output space (i.e. the number of\n",
        "      output filters in the convolution).\n",
        "    kernel_size: An integer or tuple/list of 2 integers, specifying the height\n",
        "      and width of the 2D convolution window. Can be a single integer to specify\n",
        "      the same value for all spatial dimensions.\n",
        "    strides: An integer or tuple/list of 2 integers, specifying the strides of\n",
        "      the convolution along the height and width. Can be a single integer to\n",
        "      specify the same value for all spatial dimensions. Specifying any stride\n",
        "      value != 1 is incompatible with specifying any `dilation_rate` value != 1.\n",
        "    padding: one of `\"valid\"` or `\"same\"` (case-insensitive).\n",
        "      `\"valid\"` means no padding. `\"same\"` results in padding with zeros evenly\n",
        "      to the left/right or up/down of the input such that output has the same\n",
        "      height/width dimension as the input.\n",
        "    data_format: A string, one of `channels_last` (default) or `channels_first`.\n",
        "      The ordering of the dimensions in the inputs. `channels_last` corresponds\n",
        "      to inputs with shape `(batch_size, height, width, channels)` while\n",
        "      `channels_first` corresponds to inputs with shape `(batch_size, channels,\n",
        "      height, width)`. It defaults to the `image_data_format` value found in\n",
        "      your Keras config file at `~/.keras/keras.json`. If you never set it, then\n",
        "      it will be `channels_last`.\n",
        "    dilation_rate: an integer or tuple/list of 2 integers, specifying the\n",
        "      dilation rate to use for dilated convolution. Can be a single integer to\n",
        "      specify the same value for all spatial dimensions. Currently, specifying\n",
        "      any `dilation_rate` value != 1 is incompatible with specifying any stride\n",
        "      value != 1.\n",
        "    groups: A positive integer specifying the number of groups in which the\n",
        "      input is split along the channel axis. Each group is convolved separately\n",
        "      with `filters / groups` filters. The output is the concatenation of all\n",
        "      the `groups` results along the channel axis. Input channels and `filters`\n",
        "      must both be divisible by `groups`.\n",
        "    activation: Activation function to use. If you don't specify anything, no\n",
        "      activation is applied (see `keras.activations`).\n",
        "    use_bias: Boolean, whether the layer uses a bias vector.\n",
        "    kernel_initializer: Initializer for the `kernel` weights matrix (see\n",
        "      `keras.initializers`). Defaults to 'glorot_uniform'.\n",
        "    bias_initializer: Initializer for the bias vector (see\n",
        "      `keras.initializers`). Defaults to 'zeros'.\n",
        "    kernel_regularizer: Regularizer function applied to the `kernel` weights\n",
        "      matrix (see `keras.regularizers`). \n",
        "    bias_regularizer: Regularizer function applied to the bias vector (see\n",
        "      `keras.regularizers`). \n",
        "    activity_regularizer: Regularizer function applied to the output of the\n",
        "      layer (its \"activation\") (see `keras.regularizers`).\n",
        "    kernel_constraint: Constraint function applied to the kernel matrix (see\n",
        "      `keras.constraints`).\n",
        "    bias_constraint: Constraint function applied to the bias vector (see\n",
        "      `keras.constraints`).\n",
        "  Input shape:\n",
        "    4+D tensor with shape: `batch_shape + (channels, rows, cols)` if\n",
        "      `data_format='channels_first'`\n",
        "    or 4+D tensor with shape: `batch_shape + (rows, cols, channels)` if\n",
        "      `data_format='channels_last'`.\n",
        "  Output shape:\n",
        "    4+D tensor with shape: `batch_shape + (filters, new_rows, new_cols)` if\n",
        "    `data_format='channels_first'` or 4+D tensor with shape: `batch_shape +\n",
        "      (new_rows, new_cols, filters)` if `data_format='channels_last'`.  `rows`\n",
        "      and `cols` values might have changed due to padding.\n",
        "  Returns:\n",
        "    A tensor of rank 4+ representing\n",
        "    `activation(conv2d(inputs, kernel) + bias)`.\n",
        "  Raises:\n",
        "    ValueError: if `padding` is `\"causal\"`.\n",
        "    ValueError: when both `strides > 1` and `dilation_rate > 1`.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               filters,\n",
        "               kernel_size,\n",
        "               strides=(1, 1),\n",
        "               padding='valid',\n",
        "               data_format=None,\n",
        "               dilation_rate=(1, 1),\n",
        "               groups=1,\n",
        "               activation=None,\n",
        "               use_bias=True,\n",
        "               kernel_initializer='glorot_uniform',\n",
        "               bias_initializer='zeros',\n",
        "               kernel_regularizer=None,\n",
        "               bias_regularizer=None,\n",
        "               activity_regularizer=None,\n",
        "               kernel_constraint=None,\n",
        "               bias_constraint=None,\n",
        "               **kwargs):\n",
        "    super(Conv2D, self).__init__(\n",
        "        rank=2,\n",
        "        filters=filters,\n",
        "        kernel_size=kernel_size,\n",
        "        strides=strides,\n",
        "        padding=padding,\n",
        "        data_format=data_format,\n",
        "        dilation_rate=dilation_rate,\n",
        "        groups=groups,\n",
        "        activation=activations.get(activation),\n",
        "        use_bias=use_bias,\n",
        "        kernel_initializer=initializers.get(kernel_initializer),\n",
        "        bias_initializer=initializers.get(bias_initializer),\n",
        "        kernel_regularizer=regularizers.get(kernel_regularizer),\n",
        "        bias_regularizer=regularizers.get(bias_regularizer),\n",
        "        activity_regularizer=regularizers.get(activity_regularizer),\n",
        "        kernel_constraint=constraints.get(kernel_constraint),\n",
        "        bias_constraint=constraints.get(bias_constraint),\n",
        "        **kwargs)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}